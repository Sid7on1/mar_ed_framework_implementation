{
  "agent_id": "coder2",
  "task_id": "task_1",
  "files": [
    {
      "filename": "keyframe_primitives.py",
      "purpose": "Implementation of Keyframe Primitives for keyframe detection and experience condensation.",
      "priority": "high",
      "dependencies": [
        "OpenCV",
        "SciPy"
      ],
      "key_functions": [
        "keyframe_detection",
        "experience_condensation"
      ],
      "estimated_lines": 300,
      "complexity": "medium"
    }
  ],
  "project_info": {
    "project_name": "MAR-ED Framework Implementation",
    "project_type": "computer_vision",
    "description": "Implementation of the Spatio-Temporal Mixed and Augmented Reality Experience Description (MAR-ED) framework for interactive and adaptive playback of past spatio-temporal experiences.",
    "key_algorithms": [
      "Semantic Scene Understanding",
      "Event Primitives",
      "Keyframe Primitives",
      "Playback Primitives"
    ],
    "main_libraries": [
      "OpenCV",
      "PyTorch",
      "NumPy",
      "SciPy",
      "OpenGL"
    ]
  },
  "paper_content": "PDF: cs.HC_2508.15258v1_Spatio-Temporal-Mixed-and-Augmented-Reality-Experi.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nSpatio-Temporal Mixed and Augmented Reality Experience Description for\nInteractive Playback\nDooyoung Kim *\nKAIST KI-ITC ARRCWoontack Woo\u2020\nKAIST UVR Lab.\nKAIST KI-ITC ARRC\nFigure 1: This figure illustrates the concept of a spatio-temporal Mixed and Augmented Reality Experience Description (MAR-ED) for\ninteractive playback. (A, B) An expert\u2019s drone tutorial is captured and stored as keyframes within a scene graph, enabling it to be\nadaptively replayed in different spatio-temporal contexts. The system supports playback speeds tailored to the user\u2019s pace and can\nrespond to new interactions, such as (C) user questions or proactive suggestions based on user behavior.\nABSTRACT\nWe propose the Spatio-Temporal Mixed and Augmented Reality\nExperience Description (MAR-ED), a novel framework to standard-\nize the representation of past events for interactive and adaptive\nplayback in a user\u2019s present physical space. While current spatial\nmedia technologies have primarily focused on capturing or replaying\ncontent as static assets, often disconnected from the viewer\u2019s envi-\nronment or offering limited interactivity, the means to describe an\nexperience\u2019s underlying semantic and interactive structure remains\nunderexplored. We propose a descriptive framework called MAR-\nED based on three core primitives: 1) Event Primitives for semantic\nscene graph representation, 2) Keyframe Primitives for efficient and\nmeaningful data access, and 3) Playback Primitives for user-driven\nadaptive interactive playback of recorded MAR experience. The pro-\nposed flowchart of the three-stage process of the proposed MAR-ED\nframework transforms a recorded experience into a unique adap-\ntive MAR experience during playback, where its spatio-temporal\nstructure dynamically conforms to a new environment and its narra-\ntive can be altered by live user input. Drawing on this framework,\npersonal digital memories and recorded events can evolve beyond\npassive 2D/3D videos into immersive, spatially-integrated group ex-\nperiences, opening new paradigms for training, cultural heritage, and\ninteractive storytelling without requiring complex, per-user adaptive\nrendering.\nKeywords: Augmented Reality, Mixed Reality, Spatio-Temporal\nExperience, Data Representation, Interactive Playback, XRMemory\n*e-mail: dooyoung.kim@kaist.ac.kr\n\u2020Corresponding Author. e-mail: wwoo@kaist.ac.kr1 I NTRODUCTION\nAs immersive technologies continue to blur the boundaries between\nphysical and virtual worlds, the desire to capture and relive mem-\nories is evolving beyond the limitations of traditional photos and\nvideos [6]. The future of how we record and re-experience memo-\nries points not toward passive viewing, but toward full, interactive\nimmersion. Envision entering an archival recording of a past event,\nable to converse with the figures within it or interact with key objects\nto alter outcomes, as imagined in science fiction like Ready Player\nOne [8]. This level of engagement requires that recorded memories\nare no longer static assets to be simply watched, but dynamic worlds\nto be explored and influenced. However, current spatial media tech-\nnologies fall critically short of this vision. To enable a future where\nwe can truly step into and interact with our past, a new standard for\ndescribing Mixed and Augmented Reality (MAR) experiences is not\njust beneficial, but essential.\nDespite the growing prominence of technologies for capturing\nour world in 3D, creating a compelling method to replay and interact\nwith these past experiences remains a significant design challenge.\nThe core issue is the lack of a standardized way to describe an\nexperience itself\u2014not just as a collection of visual data, but as a\nstructured narrative of meaningful events, interactions, and spatial\nrelationships. Existing formats are designed to describe assets for\npresentation [3, 5], constraining their utility for creating dynamic,\ninteractive, and personalized replays of past events. Despite the\ngrowing prominence of technologies for capturing our world in 3D,\ncreating a compelling method to replay and interact with these past\nexperiences remains a significant design challenge. The core issue\nis the lack of a standardized way to describe an experience itself,\nnot just as a collection of visual data, but as a structured narrative of\nmeaningful events, interactions, and spatial relationships.\nTo address this gap, our work introduces the Spatio-Temporal\nMixed and Augmented Reality Experience Description (MAR-ED),\na framework that defines a standard for representing past events forarXiv:2508.15258v1  [cs.HC]  21 Aug 2025\n\n--- Page 2 ---\nFigure 2: The flowchart of the three-stage process of the proposed MAR-ED (Mixed and Augmented Reality Experience Description) framework.\ninteractive and adaptive playback. This approach transforms a canon-\nical recorded experience into a unique adaptive MAR experience. As\nillustrated in Fig. 1 (Generated using Gemini 2.5 Pro [1]), this allows\na user to begin an adaptive drone tutorial by first engaging with a\nstandard, pre-recorded lesson from an expert on drone principles (A,\nB). However, when the user encounters a point of confusion and\nposes a question, the system dynamically deviates from the recorded\npath. Fig. 1(C) shows it generates a new, interactive Q&A session in\nreal-time to address the user\u2019s specific query, demonstrating a truly\nresponsive learning environment. Once the question is resolved,\nFig. 1\u2019s timeline illustration shows the system seamlessly transitions\nback to the main tutorial, adapting to the user\u2019s pace. The time-\nline in the Fig. 1 visually contrasts this adaptive process against a\nlinear recording, highlighting the non-linear path created by user\ninteraction and the potential for dynamic playback speeds.\nThis dynamic capability is realized through three founda-\ntional sets of descriptive primitives. First, Event Primitives pro-\nvide the semantic backbone; a SemanticExperienceSegment\ndefines the high-level context, like \u201cdrone assembly,\u201d while\nInteractionEvent andStateChangeEvent primitives capture\nthe detailed, meaningful actions and their resulting state changes.\nSecond, Keyframe Primitives identify the most significant moments\nby evaluating criteria for what constitutes a KeyInteraction or\nKeyStateChange , using a KeyframeDecisionThreshold to es-\ntablish anchors for adaptation. Finally, Playback Primitives provide\nthe runtime logic for this interactivity; CreateNewBranch manages\nthe deviation for user queries, ReturnToMain ensures a coherent\ntransition back to the core narrative, and TimelineAdaptation\nadjusts the experience\u2019s timing and spatial layout.\nThe contributions of this paper are threefold. First, we propose a\nnovel framework, MAR-ED, for describing spatio-temporal expe-\nriences based on semantic events rather than raw data. Second, we\nintroduce the concept of event primitives, keyframe primitives, and\nplayback primitives that enable user-driven adaptation and interac-\ntion, transforming passive viewing into an active experience. Lastly,\nwe outline a new personal and shared media paradigm that bridges\nthe gap between past recordings and present contexts, enabling more\naccessible and immersive group experiences.\n2 RELATED WORK\nExisting scene description standards have primarily focused on the\nefficient, often frame-by-frame, representation of scenes for faithful\nreproduction. The most relevant standard, MPEG-I Scene Descrip-\ntion (ISO/IEC 23090-14) [4], excels at the efficient storage andsynchronized transmission of timed media assets for presentation.\nHowever, this asset-centric model is designed for high-fidelity replay\nof the scene as it was recorded. It inherently lacks the deep seman-\ntic structure required for enabling temporally adaptive playback in\na new space and supporting meaningful user interaction with the\nrecorded events themselves.\nFurthermore, a significant challenge lies in the adaptation of a\nrecorded experience to a new context. Current standards define\nanimation and timelines rigidly. While powerful for pre-authored\ncontent, these mechanisms are not designed to dynamically adapt\na recorded spatial trajectory or temporal sequence to a different\nphysical environment. Frameworks like MPEG-21 Digital Item\nAdaptation [2] have addressed content adaptation in a broader sense,\nand OGC standards like IndoorGML [7] can describe the new spatial\ncontext, but a dedicated mechanism to extract semantically signifi-\ncant keyframes from the original experience and use them as anchors\nfor spatio-temporal adaptation is underexplored.\nFinally, while the foundations for interactivity are well-\nestablished, a higher-level framework for managing interactive nar-\nratives is needed. Runtime APIs like OpenXR [9] provide the neces-\nsary interface to hardware for tracking and user input, and standards\nlike MPEG-V [3] can describe the capabilities of interaction devices.\nHowever, these standards provide the low-level pipes for interac-\ntion data, not the protocol for handling its consequences within\na recorded narrative. A general-purpose state machine language\nlike W3C\u2019s SCXML (State Chart XML) [10] offers a model for\nevent-driven state transitions, but it is not specialized for the unique\nchallenge of branching a spatio-temporal media experience and then\nreturning to the canonical timeline. A new descriptive capability is\nrequired to define how to respond to user interactions and manage\nthe flow between the original recorded path and new, user-driven\nevent branches.\n3 METHODOLOGY\nMixed and Augmented Reality Experience Description (MAR-ED)\nis built upon three foundational pillars, each represented by a set\nof descriptive primitives designed to capture, structure, and replay\nan experience. Fig. 2 shows the overall flow of a spatio-temporal\nexperience recording with MAR-ED and adaptive playback of the\nMAR experience. There are three primitives for the MAR-ED. In the\nfirst stage, raw spatio-temporal data is taken as input and transformed\ninto a structured semantic log through SemanticExperienceSegment,\nInteractionEvent, and StateChangeEvent. The second stage filters\nthe structured log. Based on KeyInteraction and KeyStateChange\n\n--- Page 3 ---\ncriteria, along with the KeyframeDecisionThreshold, the significance\nof each frame is assessed to generate keyframe-based experience data\n(Keyframed MAR-ED) that contains only the most crucial moments.\nIn the final stage, user-driven adaptive playback is performed based\non the distilled data. The experience is replayed along a main\ntimeline, but when a user interaction occurs, a new branch is created\n(Create New Branch) to respond in real-time. Once the interaction is\ncomplete, the system seamlessly transitions back (Return to Main)\nto the original timeline, providing an uninterrupted experience.\n3.1 Event Primitives: Semantic Scene Representation\nThese primitives form the semantic backbone of a recorded experi-\nence by structuring it as a narrative of meaningful events rather than\na stream of raw data. The relationship between them is hierarchical:\naSemanticExperienceSegment acts as a high-level container that\nprovides overall context (the \u201cwhy\u201d), InteractionEvent primi-\ntives detail the specific actions or verbs that occur within that seg-\nment (the \u201cwhat\u201d), and StateChangeEvent primitives document\nthe resulting changes to users and objects, providing descriptive\ndetails for those actions (the \u201chow\u201d). Together, they build a rich,\ninterpretable representation of the past event.\n\u2022SemanticExperienceSegment: This primitive functions\nas a high-level narrative container, defining a distinct, goal-\noriented chapter of the overall experience. It encapsulates the\nsegment\u2019s scope, including its start and end times, its primary\npurpose or task (e.g., \u201cdrone lifting method\u201d), and the key par-\nticipants and objects involved. Grouping related events under\na single semantic umbrella provides the necessary context to\ninterpret the purpose of individual actions and state changes\nthat occur within that segment.\n\u2022InteractionEvent: This primitive serves as the \u201cverb\u201d of\nthe experience, capturing the specifics of a single, meaningful\naction performed by a user. It provides a structured log that\ndetails the connection between an actor and an object, con-\ntaining not just the type of action (e.g., grasp ,press ) but\nalso the semantic context of the interaction. This includes the\nstate of the interacted object before and after the action (e.g.,\nits relational change from being on(Table) toin(Hand) ),\nand a precise timestamp. This detailed, event-centric log is\nfundamental for enabling queries about past actions and for\ntriggering responses during an interactive playback.\n\u2022StateChangeEvent: This primitive documents the signifi-\ncant changes in the properties of users or objects, often oc-\ncurring as a direct consequence of an InteractionEvent .\nIt captures the continuous or discrete changes in state, pro-\nviding the data needed for accurate visualization. This goes\nbeyond simple position to include orientation, posture for users,\nand intrinsic properties for objects (e.g., a light\u2019s isonsta-\ntus changing from false totrue ). For example, during a\ngrasp interaction, this primitive would log the trajectory and\ntransformational changes of both the user\u2019s hand and the ob-\nject from the start of the action to its completion, effectively\ndocumenting the outcome of the interaction.\n3.2 Keyframe Primitives: Efficient and Meaningful Ac-\ncess\nNot all events hold equal importance. This set of primitives defines\na mechanism to distill the continuous flow of events into a discrete\nset of keyframes, which serve as semantic anchors for efficient\nstorage and adaptive playback. This is achieved by first establishing\nthe criteria for what constitutes a significant KeyInteraction or\nKeyStateChange . The KeyframeDecisionThreshold then acts\nas a configurable filter that applies these criteria, allowing the system\nto record experiences tailored to specific purposes by flagging only\nthe most relevant moments as keyframes.\u2022KeyInteraction: It is identified based on a set of criteria\nthat distinguish semantically significant actions from minor\nmovements within a recorded experience. The primary factor\nis action semantics, where goal-oriented behaviors such as\ngrasping an object, activating a device, or giving an item to\nanother person are prioritized over ambiguous gestures. The\nsignificance of the interacted object is also crucial; an interac-\ntion with a primary task-related tool is more significant than\none with a background element. Furthermore, an action\u2019s im-\nportance is elevated if it contributes to narrative progression,\nsuch as picking up a key to open a door, which advances the\nexperience to a new stage. Finally, the social context is a key\ndeterminant, with interactions involving or directed at another\nuser, like a handshake, being inherently more significant than\nsolitary actions.\n\u2022KeyStateChange : It is defined by criteria that identify mean-\ningful and abrupt shifts in the state of a user or object, often\nresulting from a KeyInteraction . These criteria include the\nmagnitude and velocity of physical change, where large or\nsudden movements, such as a user standing up from a chair\nor an object being quickly moved, are considered key. More\nimportantly, a change in semantic relationships is a critical\nindicator; this occurs when an object\u2019s logical connection to\nits environment is altered, for instance, by being placed inside\na container, lifted off a surface, or brought into close proximity\nwith another key object. Lastly, a change in an object\u2019s intrin-\nsic state, such as its power turning on or off, or a door opening\nor closing, constitutes a definitive key state change.\n\u2022KeyframeDecisionThreshold: This is a constant value be-\ntween 0and1that sets the criteria for what qualifies as a\nkeyframe, based on the information from KeyInteraction\nandKeyStateChange primitives. By adjusting this threshold,\nit becomes possible to record MAR Experiences tailored to\nspecific objectives. A value of 0signifies that every frame is\nrecorded as a keyframe, whereas a value of 1signifies that no\nkeyframes are recorded at all.\n3.3 Playback Primitives: User-Driven Adaptive Playback\nThis component provides the runtime logic that transforms a static\nrecorded experience into a dynamic and interactive adaptive MAR\nexperience. The primitives work in concert to manage the session\u2019s\nflow: TimelineAdaptation provides the foundational adjustment\nof the experience\u2019s timing, adapting the pace to the user\u2019s context\nand needs. Building on this adapted baseline, CreateNewBranch\nandReturnToMain collaboratively handle the lifecycle of live user\ninteractions, managing the deviation from and seamless return to the\ncore recorded narrative. These primitives are the engine that drives\nthe interactive nature of the playback.\n\u2022TimelineAdaptation: This primitive is responsible for dy-\nnamically adjusting the temporal structure of the recorded ex-\nperience to adapt to the user\u2019s pace and context. The primary\nprocess is temporal scaling, which modifies the duration of\nrecorded events. For example, the playback speed of a tutorial\nsegment can be slowed down after a user asks a question, pro-\nviding them with more time to apply their new understanding.\nThis ensures the experience feels responsive and is tailored\nto the individual\u2019s learning and experience curve, rather than\nadhering to a rigid, pre-recorded timeline.\n\u2022CreateNewBranch: This primitive handles live user inter-\nactions that deviate from the original recorded narrative. It\ncaptures and interprets input from the playback user (such as\ngestures, speech, or gaze) and translates it into a recognized\nnew event or intent. Upon recognizing a valid interaction, this\n\n--- Page 4 ---\nprimitive generates a new event sequence, creating a narrative\nbranch that diverges from the main timeline. For instance, if a\nuser speaks to a recorded avatar, this primitive initiates a \u201cask\nabout lifting drone\u201d event, temporarily pausing the original\nsequence of events to allow for the new interaction.\n\u2022ReturnToMain: This primitive manages the state and logic\nfor concluding a user-created branch and seamlessly transition-\ning the experience back to the original timeline. It continu-\nously assesses the status of the interactive session, including\nthe active event segment and the user\u2019s level of engagement.\nOnce the conditions for returning are met (e.g., the conversa-\ntion ends), it identifies the most logical point in the original\nrecorded experience to resume playback, ensuring narrative\ncoherence and a smooth transition from the interactive branch\nback to the main story.\n4 DISCUSSION AND CONCLUSION\nThe MAR-ED framework represents a paradigm shift from the\npassive replay of recorded media to an active, co-creative re-\ninterpretation of past experiences. By prioritizing semantic un-\nderstanding and adaptive logic, it opens up numerous possibilities\nwhile also presenting significant technical challenges. The primary\nadvantage of this approach is the creation of deeply personalized\nand context-aware media. Applications extend far beyond personal\nmemories. In education and training, a surgeon\u2019s procedure could\nbe replayed and interactively explored in a trainee\u2019s lab. In cultural\nheritage, historical events could be reconstructed on-site for tourists\nto engage with. In entertainment, it enables interactive narratives\nthat adapt to the viewer\u2019s own home and actions.\nHowever, the practical realization of MAR-ED is contingent on\nadvances in several key technology areas. The creation of a recorded\nexperience requires robust AI-driven semantic scene understanding\nto automatically identify events, actions, and states from sensor\ndata in real-time. Generating an adaptive MAR experience in turn\ndemands sophisticated user intent recognition to interpret ambiguous\ninputs, as well as highly accurate SLAM and spatial mapping to\nreliably anchor and adapt the experience to a new environment. The\nMAR-ED framework itself is a high-level proposal. Future work\nmust define a concrete data format (e.g., as an extension to glTF),\ndevelop a formal ontology for semantic events, and create a reference\nimplementation to validate its feasibility. A tiered approach, defining\nprofiles from simple playback to fully interactive adaptation, could\nfacilitate gradual industry adoption. These challenges underscore\nthat MAR-ED is not just a new file format, but a roadmap for a new\nclass of intelligent, context-aware media.\nIn this paper, we introduced the MAR-ED, a framework de-\nsigned to enable the interactive and adaptive playback of past spatio-\ntemporal experiences. By structuring experiences around semantic\nprimitives for events, keyframes, and playback logic, MAR-ED pro-\nvides a method to transform static, immutable recorded experiences\ninto dynamic, personalized, and adaptive MAR experiences. This\napproach bridges the gap between past recordings and the user\u2019s\npresent context, allowing recorded narratives to be seamlessly in-\ntegrated into new physical spaces and altered by live interaction.\nWe detailed the core components of the framework, positioned it\nagainst existing standards, and discussed its potential applications\nand technical challenges. While significant research is needed to\nfully realize its potential, MAR-ED offers a foundational blueprint\nfor the future of personal media, moving us toward a world where\nour digital memories are not just watched, but truly re-experienced.\nACKNOWLEDGMENTS\nThis work was supported by the Institute of Information & commu-\nnications Technology Planning & Evaluation (IITP) grant funded by\nthe Korea government (MSIT) (No. RS-2024-00397663, Real-timeXR Interface Technology Development for Environmental Adapta-\ntion).\nREFERENCES\n[1]Google. Gemini. https://gemini.google.com/ , July 2025. [Text-\nto-Image generation from Gemini 2.5 Pro model]. Image generated on\nJuly 18, 2025.\n[2]International Organization for Standardization. Information technology\n\u2013 multimedia framework (mpeg-21) \u2013 part 7: Digital item adaptation.\nStandard ISO/IEC 21000-7:2004, ISO/IEC, 2004.\n[3]International Organization for Standardization. Information technology\n\u2013 media context and control \u2013 part 1: Architecture. Standard ISO/IEC\n23005-1:2016, ISO/IEC, 2016.\n[4]International Organization for Standardization. Information technology\n\u2013 coded representation of immersive media \u2013 part 14: Scene description.\nStandard ISO/IEC 23090-14:2022, ISO/IEC, 2022.\n[5]International Organization for Standardization. Information technol-\nogy \u2013 computer graphics, image processing and environmental data\nrepresentation \u2013 extensible 3d (x3d) \u2013 part 1: Architecture and base\ncomponents. Standard ISO/IEC 19775-1:2023, ISO/IEC, 2023.\n[6]D. Kim, K. Kim, C. T. Silva, Q. Sun, D. Turakhia, Z. Wang, K. Wang,\nK. Perlin, S. Feiner, and W. Woo. An overview of the 1st international\nworkshop on spatial memory in xr: The future of memory capture\nand replay through xr and ai (xrmemory). In 2025 IEEE Conference\non Virtual Reality and 3D User Interfaces Abstracts and Workshops\n(VRW) , pp. 1132\u20131133. IEEE, 2025.\n[7]Open Geospatial Consortium. Ogc indoorgml 1.0. Standard 12-005r4,\nOGC, 2014.\n[8]S. Spielberg. Ready player one. Film, 2018. Directed by Steven\nSpielberg.\n[9]The Khronos Group. Openxr: The open standard for vr and ar. https:\n//www.khronos.org/openxr/ , 2025. Accessed: August 12, 2025.\n[10] W3C. State chart xml (scxml): State machine notation for control\nabstraction. W3c recommendation, W3C, September 2015.",
  "project_dir": "artifacts/projects/MAR-ED Framework Implementation",
  "communication_dir": "artifacts/projects/MAR-ED Framework Implementation/.agent_comm",
  "assigned_at": "2025-08-24T20:44:15.279182",
  "status": "assigned"
}